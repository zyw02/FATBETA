# FAT模型搜索策略设计方案

## 1. 问题分析

### 当前搜索策略的局限性
1. **只考虑正常情况下的准确率和BitOPs**
   - 没有考虑故障注入下的容错能力
   - FAT训练的模型优势没有被充分利用

2. **所有层使用相同的优化目标**
   - 没有考虑不同层对故障的敏感性差异
   - 可能为不敏感的层分配了过高的bit

3. **没有利用FAT训练带来的容错性优势**
   - FAT训练的模型对不同bit配置可能有不同的容错性表现
   - 应该选择在故障下表现更好的bit配置

## 2. 方案设计

### 方案1：多目标优化搜索（⭐推荐）

**核心思想**：同时优化准确率、容错性和BitOPs

**目标函数**：
```
综合评分 = α × 准确率 + β × 容错性 - γ × BitOPs
```

**实现细节**：
1. 在评估每个候选bit配置时，同时计算：
   - 正常准确率：`acc_normal`
   - 故障准确率：`acc_fault`（在BER=2e-2或3e-2下）
   - 容错性得分：`fault_tolerance = acc_fault / acc_normal`（或相对下降）
   - BitOPs：`bitops`

2. 综合评分：
   ```
   score = α * normalized_acc_normal + β * normalized_fault_tolerance - γ * normalized_bitops
   ```

3. 权重建议：
   - α = 0.5（正常准确率）
   - β = 0.3（容错性）
   - γ = 0.2（BitOPs惩罚）
   - 可根据需求调整

**优点**：
- ✅ 同时优化多个目标
- ✅ 权重可配置
- ✅ 符合FAT模型的目标

**缺点**：
- ⚠️ 需要故障注入评估，计算成本增加
- ⚠️ 权重需要调优

---

### 方案2：分层容错敏感度分析

**核心思想**：不同层对故障的敏感性不同，敏感层使用更高bit

**实现思路**：
1. **预分析阶段**：
   - 对每个层单独进行故障注入测试
   - 记录每层在故障下的准确率下降
   - 计算敏感度分数：`sensitivity[layer] = (acc_normal - acc_fault) / acc_normal`

2. **搜索阶段**：
   - 为敏感层分配更高的bit权重
   - 修改优化目标：`score = accuracy - bitops_penalty * layer_importance`
   - `layer_importance`基于敏感度计算

**优点**：
- ✅ 针对性强，能够识别关键层
- ✅ 计算效率高（敏感度可预计算）

**缺点**：
- ⚠️ 需要额外的预分析步骤
- ⚠️ 敏感度可能随bit配置变化

---

### 方案3：故障感知的贪婪搜索（⭐快速实现）

**核心思想**：修改现有贪婪搜索，加入故障注入评估

**实现思路**：
1. 修改`forward_loss`函数：
   - 在评估候选配置时，同时进行正常和故障注入评估
   - 返回：`(loss_normal, loss_fault, acc_normal, acc_fault)`

2. 修改优化目标：
   - 原始：`score = acc_scale * normalized_acc - normalized_complexity`
   - 新目标：`score = acc_scale * (α * acc_normal + β * acc_fault) - normalized_complexity`

3. 可选：自适应BER
   - 开始时使用较小BER（如1e-2）
   - 逐步增大到目标BER（如2e-2或3e-2）

**优点**：
- ✅ 基于现有代码，实现简单
- ✅ 改动最小

**缺点**：
- ⚠️ 每次评估需要两次forward（正常+故障）
- ⚠️ 搜索时间增加约2倍

---

### 方案4：两阶段搜索（⭐效率与效果平衡）

**核心思想**：先快速筛选，再精细评估容错性

**第一阶段：快速筛选（基于BitOPs + 正常准确率）**
- 快速筛选出Top-K（如50）候选配置
- 只评估正常准确率和BitOPs
- 目标：快速缩小搜索空间

**第二阶段：容错性评估（基于故障注入）**
- 对第一阶段筛选出的候选配置进行容错性评估
- 使用故障注入测试每个候选配置
- 综合排序：`score = α * acc_normal + β * fault_tolerance - γ * bitops`
- 返回Top-N（如5-10）最佳配置

**优点**：
- ✅ 平衡效率和效果
- ✅ 可控制计算成本
- ✅ 灵活性高

**缺点**：
- ⚠️ 需要两阶段实现
- ⚠️ 可能错过第一阶段未筛选但容错性好的配置

---

### 方案5：渐进式容错搜索

**核心思想**：分阶段逐步提高对容错性的要求

**实现思路**：
1. **阶段1（前50%迭代）**：主要优化BitOPs和正常准确率
   - 目标：`score = acc_normal - bitops_penalty`

2. **阶段2（后50%迭代）**：加入容错性考虑
   - 目标：`score = 0.7 * acc_normal + 0.3 * fault_tolerance - bitops_penalty`

3. **阶段3（最终20%迭代）**：重点优化容错性
   - 目标：`score = 0.5 * acc_normal + 0.5 * fault_tolerance - 0.3 * bitops_penalty`

**优点**：
- ✅ 渐进式优化，稳定
- ✅ 兼顾效率和效果

**缺点**：
- ⚠️ 需要定义阶段边界
- ⚠️ 参数较多

---

## 3. 推荐方案组合

### 主要方案：方案1（多目标优化）+ 方案4（两阶段搜索）

**工作流程**：
1. **第一阶段**：快速筛选候选配置（基于BitOPs + 正常准确率）
2. **第二阶段**：对候选配置进行故障注入评估，使用多目标优化排序

### 辅助方案：方案2（分层敏感度分析）

- 用于预分析和权重设置
- 为不同层设置不同的bit权重

---

## 4. 配置参数建议

```yaml
fault_aware_search:
  enabled: true
  
  # 多目标优化权重
  weights:
    alpha: 0.5      # 正常准确率权重
    beta: 0.3       # 容错性权重
    gamma: 0.2      # BitOPs惩罚权重
  
  # 故障注入评估配置
  fault_injection:
    ber: 2e-2       # 评估时使用的BER（与FAT训练匹配）
    sample_batches: 5  # 评估时使用的batch数量（减少计算）
  
  # 两阶段搜索配置
  two_stage:
    enabled: true
    stage1_topk: 50    # 第一阶段筛选的候选数量
    stage2_topk: 10    # 第二阶段最终返回的配置数量
  
  # 分层敏感度分析（可选）
  layer_sensitivity:
    enabled: false
    pre_analysis_batches: 10  # 预分析使用的batch数量
```

---

## 5. 实现复杂度评估

| 方案 | 实现复杂度 | 计算成本 | 效果预期 |
|------|-----------|---------|---------|
| 方案1 | 中等 | 高（每次评估需要故障注入） | ⭐⭐⭐⭐⭐ |
| 方案2 | 较高 | 中（预分析+正常搜索） | ⭐⭐⭐⭐ |
| 方案3 | 较低 | 高（每次评估需要故障注入） | ⭐⭐⭐⭐ |
| 方案4 | 中等 | 中（两阶段，可控制） | ⭐⭐⭐⭐⭐ |
| 方案5 | 较低 | 中（阶段化评估） | ⭐⭐⭐⭐ |

---

## 6. 实施建议

### 阶段1：快速验证（方案3）
- 最小修改现有代码
- 快速验证容错性搜索的有效性
- 时间：1-2天

### 阶段2：优化实现（方案1）
- 实现多目标优化
- 验证权重配置
- 时间：2-3天

### 阶段3：效率优化（方案4）
- 实现两阶段搜索
- 提升搜索效率
- 时间：2-3天

### 阶段4：精细优化（方案2）
- 实现分层敏感度分析
- 进一步优化搜索效果
- 时间：3-5天

---

## 7. 预期效果

### 与现有搜索策略对比

| 指标 | 现有策略 | FAT搜索策略 |
|------|---------|------------|
| 正常准确率 | 基准 | 可能略降（-1~2%） |
| 故障准确率 | 基准 | 显著提升（+5~10%） |
| BitOPs | 基准 | 相似或略高 |
| 搜索时间 | 基准 | 增加50~100% |

### 关键优势
1. **容错性提升**：在故障注入下，准确率保持更好
2. **更适合FAT模型**：充分利用FAT训练带来的容错性优势
3. **实际应用价值**：在SEU环境下更可靠

---

## 8. 后续优化方向

1. **自适应BER**：根据训练时的BER动态调整评估BER
2. **多BER评估**：同时评估多个BER值，选择最鲁棒的配置
3. **在线学习**：在搜索过程中学习层敏感度
4. **神经网络搜索**：使用NAS方法自动搜索最优配置



