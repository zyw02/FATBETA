"""
Fault Injection Tool for Retraining-Free Quantization

This module provides a fault injection tool specifically designed for the 
retraining-free-quantization framework. It supports:
- BER (Bit-Error-Rate) mode fault injection
- Automatic reading of quantization bit-width configurations from layers
- Training and inference mode support
- GPU-accelerated bit-flip operations
- Gradient-preserving fault injection
- Loading bit-width configurations from JSON files for mixed-precision models

Isolation: This tool uses hook-based wrapping that can be enabled/disabled 
without affecting the original model behavior.
"""

import math
import json
import os
import hashlib
from typing import Optional, Literal, Dict, Any, Tuple, List
import torch
import torch.nn as nn
from prettytable import PrettyTable
from quan.func import QuanConv2d, QuanLinear
from .qat import set_bit_width, get_quantized_layers


Mode = Literal["ber"]


class FaultInjector:
    """
    Fault injector for quantized weights in retraining-free-quantization models.
    
    âš ï¸ **é‡è¦è¯´æ˜ï¼šæ•…éšœç±»å‹èŒƒå›´**
    - å½“å‰åªæ¨¡æ‹Ÿ**æ•°æ®ä½ç¿»è½¬**ï¼ˆData Bit Flipsï¼‰ï¼šæƒé‡å­˜å‚¨å™¨çš„SEU
      - é€‚ç”¨äºï¼šFPGA BRAMã€ASIC SRAMã€GPUå†…å­˜ä¸­çš„æƒé‡æ•°æ®
      - å½±å“ï¼šæ•°æ®å€¼æ”¹å˜ï¼Œä½†ç”µè·¯ç»“æ„ä¸å˜
    - ä¸æ¨¡æ‹Ÿ**é…ç½®ä½ç¿»è½¬**ï¼ˆConfiguration Bit Flipsï¼‰ï¼šFPGAé…ç½®å­˜å‚¨å™¨çš„SEU
      - é…ç½®ä½ç¿»è½¬ä¼šå¯¼è‡´ç”µè·¯è·¯ç”±é”™è¯¯ã€é€»è¾‘åŠŸèƒ½æ”¹å˜
      - å¯èƒ½å¯¼è‡´ç”µè·¯å®Œå…¨å¤±æ•ˆ
      - éœ€è¦FPGAæ¯”ç‰¹æµä¿¡æ¯å’Œé‡æ–°éƒ¨ç½²ï¼Œä¸åœ¨å½“å‰æ¨¡æ‹ŸèŒƒå›´å†…
    
    Features:
    - Works for both inference and training (preserves gradients)
    - BER mode: per-bit Bernoulli flips with probability `ber`
    - Automatically reads quantization bit-width from layer configuration
    - Only applies to weights; activation and other params are untouched
    - Isolation: enable()/disable() wraps and restores forward methods
    
    Args:
        model: The quantized model (should contain QuanConv2d/QuanLinear layers)
        mode: Injection mode, currently only "ber" is supported
        ber: Bit-error-rate probability (0.0 to 1.0)
        device: Device for fault injection (default: model's device)
        enable_in_training: If True, enable fault injection during training
        enable_in_inference: If True, enable fault injection during inference
        seed: Random seed for reproducibility
    
    See also:
        - FPGA_FAULT_TYPES.md: è¯¦ç»†è¯´æ˜FPGAä¸­é…ç½®ä½ç¿»è½¬ vs æ•°æ®ä½ç¿»è½¬çš„åŒºåˆ«
        - SPACEBORNE_FAULT_MODEL.md: æ˜Ÿè½½å¹³å°æ•…éšœæ¨¡å‹åˆ†æ
    """
    
    def __init__(
        self,
        model: torch.nn.Module,
        mode: Mode = "ber",
        ber: Optional[float] = None,
        device: Optional[torch.device] = None,
        enable_in_training: bool = True,
        enable_in_inference: bool = True,
        seed: Optional[int] = None,
        use_position_based_mask: bool = False,
        seed_list: Optional[List[int]] = None,
        skip_first_last: bool = False,
        use_random_flip_in_training: bool = False,
        enable_statistics: bool = False,  # æ˜¯å¦å¯ç”¨ç»Ÿè®¡åŠŸèƒ½ï¼ˆé»˜è®¤å…³é—­ä»¥æå‡æ€§èƒ½ï¼‰
        whitelist_layer: Optional[str] = None,  # ä»…é’ˆå¯¹ç‰¹å®šå±‚è¿›è¡Œæ•…éšœæ³¨å…¥ï¼ˆç”¨äºæ•æ„Ÿåº¦åˆ†æï¼‰
        gray_code_layers: Optional[List[str]] = None,  # ä½¿ç”¨æ ¼é›·ç ç¼–ç çš„å±‚åˆ—è¡¨ï¼ˆNoneè¡¨ç¤ºä¸ä½¿ç”¨æ ¼é›·ç ï¼‰
        olm_layers: Optional[Dict[str, Dict[int, int]]] = None,  # ä½¿ç”¨OLMç¼–ç çš„å±‚æ˜ å°„ {layer_name: {value: code}}
    ) -> None:
        self.model = model
        self.mode = mode
        self.ber = ber
        self.device = device
        self.enable_in_training = enable_in_training
        self.enable_in_inference = enable_in_inference
        self.seed = seed
        self.use_position_based_mask = use_position_based_mask  # æ˜¯å¦ä½¿ç”¨åŸºäºä½ç½®çš„å›ºå®šæ©ç 
        self.seed_list = seed_list  # å›ºå®šçš„seedåˆ—è¡¨ï¼Œè®­ç»ƒæ—¶è½®è¯¢ä½¿ç”¨ï¼ŒéªŒè¯æ—¶ä»ä¸­éšæœºé‡‡æ ·
        self.use_random_flip_in_training = use_random_flip_in_training  # è®­ç»ƒæ—¶æ˜¯å¦ä½¿ç”¨å®Œå…¨éšæœºåŒ–çš„bit-flipï¼ˆä¸ä½¿ç”¨base_seed+hashï¼‰
        self._current_seed_index = 0  # å½“å‰ä½¿ç”¨çš„seedç´¢å¼•ï¼ˆç”¨äºè®­ç»ƒæ—¶è½®è¯¢ï¼ŒéªŒè¯æ—¶æŒ‰é¡ºåºä½¿ç”¨ï¼‰
        self._current_forward_seed = None  # å½“å‰forwardä½¿ç”¨çš„base_seedï¼ˆè®­ç»ƒæ—¶ï¼ŒåŒä¸€ä¸ªforwardä¸­æ‰€æœ‰å±‚ä½¿ç”¨ç›¸åŒçš„base_seedï¼‰
        
        # ç»Ÿè®¡æ¯ä¸ªseedçš„ä½¿ç”¨é¢‘ç‡
        self._seed_usage_count = {}  # {seed: count}
        if self.seed_list is not None:
            for s in self.seed_list:
                self._seed_usage_count[s] = 0
        
        self._wrapped: Dict[int, Any] = {}
        self._enabled = False
        self._training_state = None
        self._wrap_logged = False  # æ ‡è®°æ˜¯å¦å·²æ‰“å°åŒ…è£…æ—¥å¿—
        self._layer_name_map: Dict[int, str] = {}  # å­˜å‚¨æ¯ä¸ªmoduleçš„layeråç§°ï¼Œkeyæ˜¯id(module.quan_w_fn)ï¼ˆä»…å½“use_position_based_mask=Trueæ—¶ä½¿ç”¨ï¼‰
        self.skip_first_last = skip_first_last  # æ˜¯å¦è·³è¿‡ç¬¬ä¸€å±‚å’Œæœ€åä¸€å±‚
        self.enable_statistics = enable_statistics  # æ˜¯å¦å¯ç”¨ç»Ÿè®¡åŠŸèƒ½
        self.whitelist_layer = whitelist_layer  # ç™½åå•å±‚
        self.gray_code_layers = set(gray_code_layers) if gray_code_layers else set()  # ä½¿ç”¨æ ¼é›·ç çš„å±‚é›†åˆ
        self.olm_layers = olm_layers if olm_layers else {}  # ä½¿ç”¨OLMç¼–ç çš„å±‚æ˜ å°„ {layer_name: {value: code}}
        # ä¸ºOLMåˆ›å»ºåå‘æ˜ å°„ï¼ˆcode -> valueï¼‰ä»¥åŠ é€ŸæŸ¥æ‰¾
        self.olm_code_to_value: Dict[str, Dict[int, int]] = {}
        for layer_name, value_to_code in self.olm_layers.items():
            self.olm_code_to_value[layer_name] = {code: value for value, code in value_to_code.items()}
        
        # ç»Ÿè®¡ä¿¡æ¯ï¼šè®°å½•å®é™…ç¿»è½¬çš„bitæ•°å’Œæ€»bitæ•°
        self._flip_stats: Dict[str, Dict[str, int]] = {}  # {layer_name: {'flipped_bits': int, 'total_bits': int, 'injections': int, 'total_params': int, 'affected_params': int}}
        # å»¶è¿Ÿç»Ÿè®¡ï¼šç´¯ç§¯flip_maskçš„sumï¼Œé¿å…æ¯æ¬¡GPU-CPUåŒæ­¥
        # ä½¿ç”¨å¼‚æ­¥æ–¹å¼ï¼šå­˜å‚¨flip_maskçš„sum tensorï¼ˆä¸ç«‹å³åŒæ­¥åˆ°CPUï¼‰
        self._pending_stats: List[Tuple[torch.Tensor, int, int, str]] = []  # [(flip_mask_sum_tensor, total_bits, total_params, layer_name), ...]
        
        # Validate and convert BER to float if needed
        if self.mode == "ber":
            if self.ber is None:
                raise ValueError("BER mode requires ber parameter")
            # Convert string to float if needed (e.g., "1e-2" from YAML)
            if isinstance(self.ber, str):
                self.ber = float(self.ber)
            else:
                self.ber = float(self.ber)
            # Validate range
            if self.ber < 0 or self.ber > 1:
                raise ValueError(f"BER mode requires 0 <= ber <= 1, got {self.ber}")
        
        # Validate seed_list
        if self.seed_list is not None:
            if not isinstance(self.seed_list, (list, tuple)) or len(self.seed_list) == 0:
                raise ValueError("seed_list must be a non-empty list or tuple of integers")
            self.seed_list = [int(s) for s in self.seed_list]
            # If seed_list is provided, use the first seed as default
            if self.seed is None:
                self.seed = self.seed_list[0]
        
        if self.seed is not None:
            torch.manual_seed(self.seed)
        # Debug trace: print per-layer flip ratio once when enabled via env
        import os
        self._trace_once = os.environ.get('FI_TRACE_ONCE', '0') == '1'
        self._traced_layers = set()
    
    def enable(self) -> None:
        """Enable fault injection by wrapping layer forward methods."""
        if self._enabled:
            return
        self._wrap_modules()
        self._enabled = True
    
    def disable(self) -> None:
        """Disable fault injection by restoring original forward methods."""
        if not self._enabled:
            return
        self._restore_modules()
        self._enabled = False
        # Reset forward seed (but keep _current_seed_index to continue round-robin)
        # This ensures that each forward pass uses a different seed from seed_list
        # instead of always starting from seed_list[0] = 42
        self._current_forward_seed = None
        # NOTE: Do NOT reset _current_seed_index here, as it would cause all forwards
        # to use seed=42. Instead, let it continue from where it left off.
        
        # å¤„ç†å»¶è¿Ÿç»Ÿè®¡ï¼šåœ¨disableæ—¶æ‰¹é‡å¤„ç†pendingçš„ç»Ÿè®¡ä¿¡æ¯
        # åªåœ¨å¯ç”¨ç»Ÿè®¡åŠŸèƒ½æ—¶æ‰å¤„ç†ï¼Œé¿å…ä¸å¿…è¦çš„å¼€é”€
        if self.enable_statistics and self._pending_stats:
            self._process_pending_statistics()
    
    def _process_pending_statistics(self) -> None:
        """
        æ‰¹é‡å¤„ç†å»¶è¿Ÿç»Ÿè®¡ä¿¡æ¯ï¼Œé¿å…åœ¨æ¯æ¬¡æ³¨å…¥æ—¶éƒ½è¿›è¡ŒGPU-CPUåŒæ­¥ã€‚
        è¿™ä¸ªæ–¹æ³•åº”è¯¥åœ¨disable()æ—¶æˆ–éœ€è¦ç»Ÿè®¡æ—¶è°ƒç”¨ã€‚
        ä½¿ç”¨æ‰¹é‡åŒæ­¥ï¼Œå‡å°‘GPU-CPUåŒæ­¥æ¬¡æ•°ï¼Œæå‡æ€§èƒ½ã€‚
        """
        if not self._pending_stats:
            return
        
        # æ‰¹é‡å¤„ç†æ‰€æœ‰pendingçš„ç»Ÿè®¡ä¿¡æ¯
        # å…ˆæ”¶é›†æ‰€æœ‰éœ€è¦åŒæ­¥çš„tensorï¼Œç„¶åä¸€æ¬¡æ€§åŒæ­¥åˆ°CPU
        flip_mask_sums = [item[0] for item in self._pending_stats]
        affected_params_sums = [item[4] for item in self._pending_stats]  # æ–°å¢ï¼šå—å½±å“çš„å‚æ•°æ•°é‡
        
        if flip_mask_sums:
            # æ‰¹é‡åŒæ­¥ï¼šå°†æ‰€æœ‰tensorçš„sumç»“æœä¸€æ¬¡æ€§åŒæ­¥åˆ°CPU
            # è¿™æ ·å¯ä»¥å‡å°‘åŒæ­¥æ¬¡æ•°ï¼Œæå‡æ€§èƒ½
            # å¦‚æœtensoræ˜¯æ ‡é‡ï¼Œéœ€è¦å…ˆunsqueeze
            processed_sums = []
            for s in flip_mask_sums:
                if s.dim() == 0:
                    processed_sums.append(s.unsqueeze(0))
                else:
                    processed_sums.append(s)
            if processed_sums:
                flipped_bits_counts = torch.cat(processed_sums).cpu().tolist()
            else:
                flipped_bits_counts = []
        else:
            flipped_bits_counts = []
        
        if affected_params_sums:
            # æ‰¹é‡åŒæ­¥å—å½±å“çš„å‚æ•°æ•°é‡
            processed_affected = []
            for s in affected_params_sums:
                if s.dim() == 0:
                    processed_affected.append(s.unsqueeze(0))
                else:
                    processed_affected.append(s)
            if processed_affected:
                affected_params_counts = torch.cat(processed_affected).cpu().tolist()
            else:
                affected_params_counts = []
        else:
            affected_params_counts = []
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        for idx, (_, total_bits, total_params, stats_key, __) in enumerate(self._pending_stats):
            flipped_bits_count = int(flipped_bits_counts[idx]) if idx < len(flipped_bits_counts) else 0
            affected_params_count = int(affected_params_counts[idx]) if idx < len(affected_params_counts) else 0
            
            if stats_key not in self._flip_stats:
                self._flip_stats[stats_key] = {
                    'flipped_bits': 0, 
                    'total_bits': 0, 
                    'injections': 0,
                    'total_params': 0,
                    'affected_params': 0
                }
            self._flip_stats[stats_key]['flipped_bits'] += flipped_bits_count
            self._flip_stats[stats_key]['total_bits'] += total_bits
            self._flip_stats[stats_key]['total_params'] += total_params
            self._flip_stats[stats_key]['affected_params'] += affected_params_count
            self._flip_stats[stats_key]['injections'] += 1
        
        # æ¸…ç©ºpendingåˆ—è¡¨
        self._pending_stats.clear()
    
    def get_seed_usage_stats(self) -> dict:
        """Get statistics about seed usage frequency."""
        total = sum(self._seed_usage_count.values())
        stats = {}
        for seed, count in self._seed_usage_count.items():
            stats[seed] = {
                'count': count,
                'percentage': (count / total * 100) if total > 0 else 0.0
            }
        return stats
    
    def get_flip_statistics(self) -> Dict[str, Dict[str, Any]]:
        """
        è·å–æ•…éšœæ³¨å…¥ç»Ÿè®¡ä¿¡æ¯ã€‚
        
        Returns:
            å­—å…¸ï¼Œæ ¼å¼ä¸º {layer_name: {
                'flipped_bits': int,      # ç¿»è½¬çš„bitæ•°
                'total_bits': int,        # æ€»bitæ•°
                'injections': int,         # æ³¨å…¥æ¬¡æ•°
                'flip_ratio': float,       # æ€»ç¿»è½¬æ¯”ä¾‹ (%)
                'avg_flip_ratio': float,  # å¹³å‡æ¯æ¬¡æ³¨å…¥çš„ç¿»è½¬æ¯”ä¾‹ (%)
                'total_params': int,      # æ€»å‚æ•°æ•°é‡
                'affected_params': int,   # å—å½±å“çš„å‚æ•°æ•°é‡ï¼ˆè‡³å°‘æœ‰ä¸€ä¸ªbitè¢«ç¿»è½¬ï¼‰
                'affected_ratio': float,  # å—å½±å“å‚æ•°æ¯”ä¾‹ (%)
                'avg_affected_ratio': float  # å¹³å‡æ¯æ¬¡æ³¨å…¥çš„å—å½±å“å‚æ•°æ¯”ä¾‹ (%)
            }}
        """
        # å¤„ç†pendingçš„ç»Ÿè®¡ä¿¡æ¯
        if self._pending_stats:
            self._process_pending_statistics()
        stats = {}
        for layer_name, data in self._flip_stats.items():
            flipped = data['flipped_bits']
            total = data['total_bits']
            injections = data['injections']
            total_params = data.get('total_params', 0)
            affected_params = data.get('affected_params', 0)
            
            flip_ratio = (flipped / total * 100) if total > 0 else 0.0
            avg_flip_ratio = flip_ratio / injections if injections > 0 else 0.0
            affected_ratio = (affected_params / total_params * 100) if total_params > 0 else 0.0
            avg_affected_ratio = affected_ratio / injections if injections > 0 else 0.0
            
            stats[layer_name] = {
                'flipped_bits': flipped,
                'total_bits': total,
                'injections': injections,
                'flip_ratio': flip_ratio,  # æ€»ç¿»è½¬æ¯”ä¾‹ (%)
                'avg_flip_ratio': avg_flip_ratio,  # å¹³å‡æ¯æ¬¡æ³¨å…¥çš„ç¿»è½¬æ¯”ä¾‹ (%)
                'total_params': total_params,  # æ€»å‚æ•°æ•°é‡
                'affected_params': affected_params,  # å—å½±å“çš„å‚æ•°æ•°é‡ï¼ˆè‡³å°‘æœ‰ä¸€ä¸ªbitè¢«ç¿»è½¬ï¼‰
                'affected_ratio': affected_ratio,  # å—å½±å“å‚æ•°æ¯”ä¾‹ (%)
                'avg_affected_ratio': avg_affected_ratio,  # å¹³å‡æ¯æ¬¡æ³¨å…¥çš„å—å½±å“å‚æ•°æ¯”ä¾‹ (%)
            }
        return stats
    
    def reset_flip_statistics(self) -> None:
        """é‡ç½®æ•…éšœæ³¨å…¥ç»Ÿè®¡ä¿¡æ¯ã€‚"""
        self._flip_stats.clear()
        self._pending_stats.clear()
    
    def print_flip_statistics(self, verbose: bool = True) -> None:
        """
        æ‰“å°æ•…éšœæ³¨å…¥ç»Ÿè®¡ä¿¡æ¯ã€‚
        
        Args:
            verbose: å¦‚æœä¸ºTrueï¼Œæ‰“å°æ¯å±‚çš„è¯¦ç»†ä¿¡æ¯ï¼›å¦åˆ™åªæ‰“å°æ±‡æ€»ä¿¡æ¯
        """
        stats = self.get_flip_statistics()
        if not stats:
            print("æ•…éšœæ³¨å…¥ç»Ÿè®¡ï¼šæš‚æ— æ•°æ®ï¼ˆå¯èƒ½å°šæœªè¿›è¡Œæ•…éšœæ³¨å…¥ï¼‰")
            return
        
        print("=" * 80)
        print("æ•…éšœæ³¨å…¥ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 80)
        print(f"é…ç½®BER: {self.ber:.2e} ({self.ber * 100:.2f}%)")
        print(f"ç»Ÿè®¡å±‚æ•°: {len(stats)}")
        print("-" * 80)
        
        total_flipped = 0
        total_bits = 0
        total_injections = 0
        
        total_params = 0
        total_affected = 0
        
        if verbose:
            # ä½¿ç”¨ PrettyTable åˆ›å»ºè¡¨æ ¼
            table = PrettyTable()
            table.field_names = [
                "å±‚åç§°",
                "ç¿»è½¬bitæ•°",
                "æ€»bitæ•°",
                "æ³¨å…¥æ¬¡æ•°",
                "ç¿»è½¬æ¯”ä¾‹",
                "å¹³å‡ç¿»è½¬æ¯”ä¾‹",
                "å—å½±å“å‚æ•°",
                "æ€»å‚æ•°æ•°",
                "å—å½±å“æ¯”ä¾‹"
            ]
            # è®¾ç½®å¯¹é½æ–¹å¼
            table.align["å±‚åç§°"] = "l"
            table.align["ç¿»è½¬bitæ•°"] = "r"
            table.align["æ€»bitæ•°"] = "r"
            table.align["æ³¨å…¥æ¬¡æ•°"] = "r"
            table.align["ç¿»è½¬æ¯”ä¾‹"] = "r"
            table.align["å¹³å‡ç¿»è½¬æ¯”ä¾‹"] = "r"
            table.align["å—å½±å“å‚æ•°"] = "r"
            table.align["æ€»å‚æ•°æ•°"] = "r"
            table.align["å—å½±å“æ¯”ä¾‹"] = "r"
            # ä½¿ç”¨ç®€æ´çš„è¡¨æ ¼é£æ ¼
            table.set_style(12)  # MSWORD_FRIENDLY é£æ ¼
        
        for layer_name, data in sorted(stats.items()):
            flipped = data['flipped_bits']
            total = data['total_bits']
            injections = data['injections']
            flip_ratio = data['flip_ratio']
            avg_flip_ratio = data['avg_flip_ratio']
            affected_params = data.get('affected_params', 0)
            total_params_layer = data.get('total_params', 0)
            affected_ratio = data.get('affected_ratio', 0.0)
            
            total_flipped += flipped
            total_bits += total
            total_injections += injections
            total_params += total_params_layer
            total_affected += affected_params
            
            if verbose:
                table.add_row([
                    layer_name,
                    f"{flipped:,}",
                    f"{total:,}",
                    injections,
                    f"{flip_ratio:.4f}%",
                    f"{avg_flip_ratio:.4f}%",
                    f"{affected_params:,}",
                    f"{total_params_layer:,}",
                    f"{affected_ratio:.4f}%"
                ])
        
        # æ‰“å°æ±‡æ€»ä¿¡æ¯
        overall_ratio = (total_flipped / total_bits * 100) if total_bits > 0 else 0.0
        avg_overall_ratio = overall_ratio / total_injections if total_injections > 0 else 0.0
        overall_affected_ratio = (total_affected / total_params * 100) if total_params > 0 else 0.0
        
        if verbose:
            # æ·»åŠ æ€»è®¡è¡Œ
            table.add_row([
                "æ€»è®¡",
                f"{total_flipped:,}",
                f"{total_bits:,}",
                total_injections,
                f"{overall_ratio:.4f}%",
                f"{avg_overall_ratio:.4f}%",
                f"{total_affected:,}",
                f"{total_params:,}",
                f"{overall_affected_ratio:.4f}%"
            ])
            print(table)
        print("=" * 80)
        print(f"å®é™…ç¿»è½¬æ¯”ä¾‹: {overall_ratio:.4f}% (é…ç½®BER: {self.ber * 100:.2f}%)")
        if abs(overall_ratio - self.ber * 100) > 0.1:
            print(f"âš ï¸  è­¦å‘Šï¼šå®é™…ç¿»è½¬æ¯”ä¾‹ä¸é…ç½®BERå·®å¼‚è¾ƒå¤§ï¼")
        else:
            print(f"âœ“ å®é™…ç¿»è½¬æ¯”ä¾‹ä¸é…ç½®BERåŸºæœ¬ä¸€è‡´")
        print("=" * 80)
    
    def print_seed_usage_stats(self, logger=None):
        """Print seed usage statistics."""
        stats = self.get_seed_usage_stats()
        total = sum(self._seed_usage_count.values())
        
        if logger:
            logger.info("=" * 80)
            logger.info("ğŸ“Š Seed Usage Statistics")
            logger.info("=" * 80)
            logger.info(f"Total forward passes with fault injection: {total}")
            logger.info("-" * 80)
            logger.info(f"{'Seed':<10} {'Count':<15} {'Percentage':<15}")
            logger.info("-" * 80)
            for seed in sorted(stats.keys()):
                count = stats[seed]['count']
                pct = stats[seed]['percentage']
                logger.info(f"{seed:<10} {count:<15} {pct:>6.2f}%")
            logger.info("=" * 80)
        else:
            print("=" * 80)
            print("ğŸ“Š Seed Usage Statistics")
            print("=" * 80)
            print(f"Total forward passes with fault injection: {total}")
            print("-" * 80)
            print(f"{'Seed':<10} {'Count':<15} {'Percentage':<15}")
            print("-" * 80)
            for seed in sorted(stats.keys()):
                count = stats[seed]['count']
                pct = stats[seed]['percentage']
                print(f"{seed:<10} {count:<15} {pct:>6.2f}%")
            print("=" * 80)
    
    def reset_forward_seed(self) -> None:
        """
        Reset the current forward seed.
        
        This should be called at the beginning of each forward pass during training
        to ensure all layers in the same forward use the same base_seed.
        """
        self._current_forward_seed = None
    
    # --- Internal helpers ---
    
    def _wrap_modules(self) -> None:
        """
        Wrap quantizer forward methods to inject faults.
        
        This wraps ALL quantized layers, including:
        - Layers with dynamic bits (from search config)
        - Layers with fixed_bits (first/last layers, typically 8-bit)
        
        All these layers should receive fault injection according to their
        respective bit-width configurations.
        """
        # å¦‚æœå·²ç»åŒ…è£…è¿‡ï¼Œç›´æ¥è¿”å›ï¼ˆé¿å…é‡å¤åŒ…è£…å’Œæ—¥å¿—ï¼‰
        if len(self._wrapped) > 0:
            return
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæ ¼é›·ç å±‚é…ç½®
        if len(self.gray_code_layers) > 0:
            import sys
            print(f"[FaultInjector] Gray code layers configured: {self.gray_code_layers}", file=sys.stderr, flush=True)
        
        wrapped_count = 0
        fixed_bits_count = 0
        dynamic_bits_count = 0
        
        # Collect all quantized layers first to identify first and last
        all_quantized_layers = []
        for name, module in self.model.named_modules():
            if not isinstance(module, (QuanConv2d, QuanLinear)):
                continue
            if not self._has_quantization_enabled(module):
                continue
            all_quantized_layers.append((name, module))
        
        # Identify first and last layers
        first_layer_name = None
        last_layer_name = None
        if self.skip_first_last and len(all_quantized_layers) > 0:
            # First layer: first conv layer (usually features.0 or similar)
            for name, module in all_quantized_layers:
                if isinstance(module, QuanConv2d):
                    first_layer_name = name
                    break
            # Last layer: last linear layer (usually classifier.6 or similar)
            for name, module in reversed(all_quantized_layers):
                if isinstance(module, QuanLinear):
                    last_layer_name = name
                    break
        
        for name, module in all_quantized_layers:
            # Skip first and last layers if requested
            if self.skip_first_last:
                if name == first_layer_name or name == last_layer_name:
                    continue
            
            # If whitelist is provided, skip all other layers
            if self.whitelist_layer is not None:
                if name != self.whitelist_layer:
                    continue

            # Count layer types for debugging
            if hasattr(module, 'fixed_bits') and module.fixed_bits is not None:
                fixed_bits_count += 1
            elif hasattr(module, 'bits') and module.bits is not None:
                dynamic_bits_count += 1
            
            # Wrap the weight quantizer
            if hasattr(module, 'quan_w_fn') and module.quan_w_fn is not None:
                key = id(module.quan_w_fn)
                if key in self._wrapped:
                    continue
                
                # Store layer name for this module (only if using position-based mask)
                if self.use_position_based_mask:
                    self._layer_name_map[key] = name
                
                # Store original forward method
                orig_quan_forward = module.quan_w_fn.forward
                
                def make_quan_wrapper(quantizer_instance, module_instance, orig_fn, layer_name_str, layer_key, use_pos_mask):
                    def wrapped_quan_forward(x, bits, is_activation=False, **kwargs):
                        # Only inject faults on weights, not activations
                        if is_activation or bits is None or bits >= 32:
                            return orig_fn(x, bits, is_activation=is_activation, **kwargs)
                        
                        # Determine if we should inject faults based on training mode
                        is_training = module_instance.training
                        should_inject = (
                            (is_training and self.enable_in_training) or
                            (not is_training and self.enable_in_inference)
                        )
                        
                        # Check if we're in restorer training mode (model in eval mode but fault injection enabled)
                        # This happens when training restorer: model is in eval mode, but we want random flips
                        is_restorer_training = (
                            not is_training and self.enable_in_inference and self.use_random_flip_in_training
                        )
                        
                        if not should_inject:
                            return orig_fn(x, bits, is_activation=is_activation, **kwargs)
                        
                        # è°ƒè¯•ä¿¡æ¯ï¼šå¯¹äºæ ¼é›·ç å±‚ï¼Œæ·»åŠ è°ƒè¯•è¾“å‡º
                        is_gray_layer = layer_name_str in self.gray_code_layers
                        if is_gray_layer:
                            import sys
                            print(f"[DEBUG] Processing gray code layer: {layer_name_str}, bits={bits}, shape={x.shape}", file=sys.stderr, flush=True)
                        
                        # Call original quantization
                        # Note: For fixed_bits layers, bits will be fixed_bits[0] (e.g., 8)
                        # For dynamic layers, bits will be from the search config (e.g., 3, 4, 5, etc.)
                        x_q = orig_fn(x, bits, is_activation=is_activation, **kwargs)
                        
                        if is_gray_layer:
                            import sys
                            print(f"[DEBUG] Quantization completed for {layer_name_str}, x_q shape={x_q.shape}", file=sys.stderr, flush=True)
                        
                        # Get scale (clip_value) from quantizer
                        # The bits parameter here is the actual bit-width for this layer
                        # (8 for fixed_bits layers, or the configured value for dynamic layers)
                        try:
                            scale = quantizer_instance.get_scale(bits, detach=True)
                            if scale is None:
                                return x_q
                            
                            # Select seed for this forward pass
                            # Priority logic:
                            # 1. If use_random_flip_in_training is True and we're in training mode,
                            #    use None (completely random, no seed) for maximum randomization
                            # 2. If not in training mode AND self.seed was explicitly set (e.g., from eval_with_fault_injection.py),
                            #    use it directly (for evaluation trials with different seeds).
                            #    This ensures each trial uses a different seed.
                            # 3. Otherwise, if seed_list is provided:
                            #    - Training: round-robin (è½®è¯¢) through seed_list to ensure all seeds are used
                            #    - Inference: use seed from seed_list in order (for reproducibility)
                            # 4. Otherwise: use self.seed (original behavior)
                            # 
                            # Important: To ensure each layer gets a different mask even with the same base seed,
                            # we combine the base seed with a hash of the layer name.
                            # This ensures:
                            # 1. Determinism: same layer_name + same base_seed â†’ same mask
                            # 2. Layer diversity: different layers get different masks even with same base_seed
                            
                            # For training with random flip: use None seed for complete randomization
                            # This applies both to normal training mode and restorer training mode
                            if (is_training and self.use_random_flip_in_training) or is_restorer_training:
                                selected_seed = None  # Completely random, no seed-based determinism
                            elif not is_training and self.seed is not None:
                                # Check if seed was explicitly set (not just from seed_list default)
                                # In eval_with_fault_injection.py, we pass seed=selected_seed explicitly,
                                # and seed_list is also passed. We want to use the explicit seed.
                                # Simple heuristic: if seed_list exists and seed equals seed_list[0] and
                                # _current_seed_index is 0, it might be from seed_list default.
                                # But if _current_seed_index is 0 and we're in eval, it's likely explicit.
                                # Actually, simpler: in eval mode, if seed is set, always use it directly.
                                base_seed = self.seed
                                # Use deterministic hash (hashlib.md5) instead of Python's hash() which may vary between runs
                                layer_hash = int(hashlib.md5(layer_name_str.encode()).hexdigest()[:8], 16) % (2**31)
                                selected_seed = base_seed + layer_hash
                            elif self.seed_list is not None:
                                if is_training:
                                    # Training: round-robin through seed_list to ensure all seeds are used
                                    # Each forward pass uses the next seed in the list, cycling through all seeds
                                    # Important: All layers in the same forward should use the same base_seed
                                    if self._current_forward_seed is None:
                                        # This is the first layer in this forward pass, select a new base_seed
                                        self._current_forward_seed = self.seed_list[self._current_seed_index % len(self.seed_list)]
                                        # ç»Ÿè®¡seedä½¿ç”¨é¢‘ç‡
                                        if self._current_forward_seed in self._seed_usage_count:
                                            self._seed_usage_count[self._current_forward_seed] += 1
                                        self._current_seed_index += 1
                                    base_seed = self._current_forward_seed
                                else:
                                    # Inference: use seed from seed_list in order
                                    base_seed = self.seed_list[self._current_seed_index % len(self.seed_list)]
                                    self._current_seed_index += 1
                                
                                # Combine base_seed with layer_name hash to ensure each layer gets different mask
                                # This ensures determinism: same layer_name + same base_seed â†’ same mask
                                # Use deterministic hash (hashlib.md5) instead of Python's hash() which may vary between runs
                                layer_hash = int(hashlib.md5(layer_name_str.encode()).hexdigest()[:8], 16) % (2**31)
                                selected_seed = base_seed + layer_hash
                            else:
                                # If no seed_list, use self.seed but still combine with layer_name for diversity
                                if self.seed is not None:
                                    # Use deterministic hash (hashlib.md5) instead of Python's hash() which may vary between runs
                                    layer_hash = int(hashlib.md5(layer_name_str.encode()).hexdigest()[:8], 16) % (2**31)
                                    selected_seed = self.seed + layer_hash
                                else:
                                    selected_seed = None
                            
                            # Inject faults on quantized weights
                            # Fault injection respects the layer's bit-width:
                            # - Fixed_bits layers (first/last): 8-bit â†’ flip bits in [-128, 127] range
                            # - Dynamic layers: their configured bit-width â†’ flip bits in corresponding range
                            # Pass layer_name for:
                            # 1. Gray code layers (needed for gray code check)
                            # 2. OLM layers (needed for OLM encoding check)
                            # 3. Position-based mask (if enabled)
                            # 4. Statistics tracking (but use None for mask generation to avoid slow hash computation)
                            is_gray_layer = (self.gray_code_layers and layer_name_str in self.gray_code_layers)
                            is_olm_layer = (self.olm_layers and layer_name_str in self.olm_layers)
                            
                            if is_gray_layer or is_olm_layer:
                                # Gray code or OLM encoding needs layer_name for encoding/decoding
                                layer_name_for_mask = layer_name_str
                            elif self.use_position_based_mask:
                                layer_name_for_mask = layer_name_str  # Needed for position-based mask
                            else:
                                layer_name_for_mask = None  # Use fast random mask generation
                            
                            # Always pass layer_name for statistics (separate from mask generation)
                            layer_name_for_stats = layer_name_str
                            
                            if is_gray_layer:
                                import sys
                                print(f"[DEBUG] Calling _inject_on_quantized_tensor for {layer_name_str}...", file=sys.stderr, flush=True)
                            
                            x_faulted = self._inject_on_quantized_tensor(
                                x_q, int(bits), scale, layer_name=layer_name_for_mask, forward_seed=selected_seed, layer_name_for_stats=layer_name_for_stats
                            )
                            
                            if is_gray_layer:
                                import sys
                                print(f"[DEBUG] Fault injection completed for {layer_name_str}, preparing return...", file=sys.stderr, flush=True)
                            
                            # Optional debug: print flip ratio once per layer (è·³è¿‡ï¼Œé¿å…é˜»å¡)
                            # if self._trace_once and layer_name_str not in self._traced_layers:
                            #     try:
                            #         # Estimate flip ratio by regenerating mask with same parameters
                            #         N = (x_q.view(-1)).numel()
                            #         k_bits = int(bits)
                            #         mask = self._generate_flip_mask(N, k_bits, device=(x_q.device if self.device is None else self.device), layer_name=layer_name_arg, mask_seed=selected_seed)
                            #         flip_ratio = float(mask.float().mean().item())
                            #         print(f"[FaultInjector TRACE] layer={layer_name_str}, bits={k_bits}, ber={self.ber:.2e}, flip_ratio={flip_ratio:.4f}")
                            #     except Exception:
                            #         pass
                            #     self._traced_layers.add(layer_name_str)
                            
                            # Preserve gradients: forward uses faulted value, backward uses original
                            if is_gray_layer:
                                import sys
                                print(f"[DEBUG] Computing gradient-preserving return for {layer_name_str}...", file=sys.stderr, flush=True)
                                print(f"[DEBUG] x_faulted: shape={x_faulted.shape}, device={x_faulted.device}, dtype={x_faulted.dtype}, requires_grad={x_faulted.requires_grad}", file=sys.stderr, flush=True)
                                print(f"[DEBUG] x_q: shape={x_q.shape}, device={x_q.device}, dtype={x_q.dtype}, requires_grad={x_q.requires_grad}", file=sys.stderr, flush=True)
                            
                            # ç®€åŒ–æ¢¯åº¦ä¿ç•™è®¡ç®—ï¼Œé¿å…å¯èƒ½çš„é˜»å¡
                            # ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨åŒä¸€ä¸ªè®¾å¤‡ä¸Š
                            if x_faulted.device != x_q.device:
                                if is_gray_layer:
                                    import sys
                                    print(f"[DEBUG] Device mismatch! Moving x_faulted from {x_faulted.device} to {x_q.device}", file=sys.stderr, flush=True)
                                x_faulted = x_faulted.to(x_q.device)
                            
                            # åœ¨ eval æ¨¡å¼ä¸‹ï¼Œç›´æ¥è¿”å›æ•…éšœå€¼ï¼Œä¸éœ€è¦æ¢¯åº¦ä¿ç•™
                            # è¿™æ ·å¯ä»¥é¿å…ä¸å¿…è¦çš„è®¡ç®—å›¾æ„å»ºï¼Œæå‡æ€§èƒ½
                            if not is_training:
                                result = x_faulted
                            else:
                                # è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œä¿ç•™æ¢¯åº¦ï¼šforward ä½¿ç”¨æ•…éšœå€¼ï¼Œbackward ä½¿ç”¨åŸå§‹å€¼
                                x_faulted_detached = x_faulted.detach()
                                x_q_detached = x_q.detach()
                                diff = x_q - x_q_detached
                                result = x_faulted_detached + diff
                            
                            if is_gray_layer:
                                import sys
                                print(f"[DEBUG] Return value computed: shape={result.shape}, device={result.device}, dtype={result.dtype}, requires_grad={result.requires_grad}", file=sys.stderr, flush=True)
                                print(f"[DEBUG] Returning from wrapped_quan_forward for {layer_name_str}...", file=sys.stderr, flush=True)
                            
                            return result
                        except Exception as e:
                            # On any failure, gracefully fall back
                            # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼ˆä»…åœ¨å¯ç”¨ç»Ÿè®¡æ—¶æ‰“å°ï¼Œé¿å…æ€§èƒ½å½±å“ï¼‰
                            if self.enable_statistics and layer_name_str in self.gray_code_layers:
                                import sys
                                print(f"[FaultInjector ERROR] Layer {layer_name_str} failed: {e}", file=sys.stderr, flush=True)
                            return x_q
                    
                    return wrapped_quan_forward
                
                # Replace quantizer forward
                module.quan_w_fn.forward = make_quan_wrapper(
                    module.quan_w_fn, module, orig_quan_forward, name, key, self.use_position_based_mask
                )
                self._wrapped[key] = orig_quan_forward
                wrapped_count += 1
        
        # Log wrapped layers info (only once, never repeat)
        if wrapped_count > 0 and not self._wrap_logged:
            print(f"[FaultInjector] Wrapped {wrapped_count} layers for fault injection: "
                  f"{dynamic_bits_count} dynamic bits layers, {fixed_bits_count} fixed_bits layers "
                  f"(first/last layers will use their fixed bit-width, e.g., 8-bit)")
            self._wrap_logged = True
    
    def _restore_modules(self) -> None:
        """Restore original quantizer forward methods."""
        for module in self.model.modules():
            if isinstance(module, (QuanConv2d, QuanLinear)):
                if hasattr(module, 'quan_w_fn') and module.quan_w_fn is not None:
                    key = id(module.quan_w_fn)
                    if key in self._wrapped:
                        module.quan_w_fn.forward = self._wrapped[key]
        self._wrapped.clear()
    
    def _has_quantization_enabled(self, module: nn.Module) -> bool:
        """
        Check if module has quantization enabled.
        
        This includes layers with:
        - bits set (dynamic bit-width layers from search)
        - fixed_bits set (first/last layers with fixed 8-bit quantization)
        
        Both types of layers should be included in fault injection.
        """
        if not hasattr(module, 'quan_w_fn'):
            return False
        # Check if bits or fixed_bits is set
        # Both should be included in fault injection
        if hasattr(module, 'bits') and module.bits is not None:
            return True
        if hasattr(module, 'fixed_bits') and module.fixed_bits is not None:
            return True
        return False
    
    def _get_weight_bits(self, module: nn.Module) -> Optional[int]:
        """
        Get weight bit-width for a layer, considering bits and fixed_bits.
        
        Note: This method is kept for compatibility, but bit-width is typically
        obtained directly from the quantizer call in wrapped_quan_forward.
        """
        # Try to get from bits attribute
        if hasattr(module, 'bits') and module.bits is not None:
            wbits = module.bits[0] if isinstance(module.bits, (list, tuple)) else module.bits
            if isinstance(wbits, torch.Tensor):
                wbits = int(wbits.item())
            else:
                wbits = int(wbits)
            return wbits
        
        # Try to get from fixed_bits attribute
        if hasattr(module, 'fixed_bits') and module.fixed_bits is not None:
            wbits = module.fixed_bits[0] if isinstance(module.fixed_bits, (list, tuple)) else module.fixed_bits
            if isinstance(wbits, torch.Tensor):
                wbits = int(wbits.item())
            else:
                wbits = int(wbits)
            return wbits
        
        return None
    
    
    @staticmethod
    def _binary_to_gray(binary: torch.Tensor) -> torch.Tensor:
        """
        å°†äºŒè¿›åˆ¶ç¼–ç è½¬æ¢ä¸ºæ ¼é›·ç ï¼ˆGray Codeï¼‰- JITç¼–è¯‘åŠ é€Ÿç‰ˆæœ¬
        
        æ ¼é›·ç ç‰¹ç‚¹ï¼šç›¸é‚»ä¸¤ä¸ªç å­—åªæœ‰ä¸€ä½ä¸åŒï¼Œå•bitç¿»è½¬åªä¼šè·³åˆ°ç›¸é‚»å€¼
        
        è½¬æ¢å…¬å¼ï¼šG = B ^ (B >> 1)
        
        Args:
            binary: äºŒè¿›åˆ¶ç¼–ç çš„æ•´æ•°å¼ é‡
            
        Returns:
            æ ¼é›·ç ç¼–ç çš„æ•´æ•°å¼ é‡
        """
        # å‘é‡åŒ–æ“ä½œï¼ŒGPUåŠ é€Ÿ
        return binary ^ (binary >> 1)
    
    @staticmethod
    def _gray_to_binary(gray: torch.Tensor, k: int) -> torch.Tensor:
        """
        å°†æ ¼é›·ç è½¬æ¢å›äºŒè¿›åˆ¶ç¼–ç ï¼ˆå‘é‡åŒ–ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        è½¬æ¢æ–¹æ³•ï¼šå‘é‡åŒ–é€ä½å¼‚æˆ–
        B = G ^ (G >> 1) ^ (G >> 2) ^ ... ^ (G >> (k-1))
        
        Args:
            gray: æ ¼é›·ç ç¼–ç çš„æ•´æ•°å¼ é‡
            k: ä½å®½
            
        Returns:
            äºŒè¿›åˆ¶ç¼–ç çš„æ•´æ•°å¼ é‡
        """
        # å¯¹äºå°ä½å®½ï¼ˆk <= 8ï¼‰ï¼Œä½¿ç”¨å¾ªç¯æ˜¯é«˜æ•ˆçš„
        # ä½¿ç”¨åŸåœ°æ“ä½œå’Œå‘é‡åŒ–ä¼˜åŒ–æ€§èƒ½
        binary = gray.clone()
        # å‘é‡åŒ–è½¬æ¢ï¼šB = G ^ (G >> 1) ^ (G >> 2) ^ ... ^ (G >> (k-1))
        # é™åˆ¶æœ€å¤§å¾ªç¯æ¬¡æ•°ï¼Œé€šå¸¸ k <= 8
        max_shift = min(k, 16)
        for i in range(1, max_shift):
            binary ^= (gray >> i)
        return binary
    
    def _inject_on_quantized_tensor(
        self, x_q: torch.Tensor, k: int, scale: torch.Tensor, layer_name: Optional[str] = None, forward_seed: Optional[int] = None, layer_name_for_stats: Optional[str] = None
    ) -> torch.Tensor:
        """
        Inject bit-flip faults on a quantized tensor using LSQ quantization format.
        
        LSQ quantization: x_q = round(x / s) * s, where x is clamped to [thd_neg * s, thd_pos * s]
        Integer code: code = round(x_q / s), which ranges from [thd_neg, thd_pos]
        
        Args:
            x_q: Quantized tensor (float values after LSQ quantization)
            k: Bit-width for quantization
            scale: Scale parameter (s) from LSQ quantizer
            layer_name: Optional layer name for statistics tracking
            forward_seed: Optional seed for this forward pass
            
        Returns:
            Faulted tensor with same shape as x_q
        """
        device = x_q.device if self.device is None else self.device
        
        # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æ ¼é›·ç æˆ–OLMç¼–ç ï¼ˆä¼˜åŒ–ï¼šå…ˆæ£€æŸ¥é•¿åº¦ï¼‰
        use_gray_code = (len(self.gray_code_layers) > 0 and 
                        layer_name is not None and 
                        layer_name in self.gray_code_layers)
        use_olm = (len(self.olm_layers) > 0 and 
                  layer_name is not None and 
                  layer_name in self.olm_layers)
        
        # æ ¼é›·ç å’ŒOLMä¸èƒ½åŒæ—¶ä½¿ç”¨
        if use_gray_code and use_olm:
            raise ValueError(f"Layer {layer_name} cannot use both Gray Code and OLM encoding")
        
        # Handle scale as tensor or scalar
        if isinstance(scale, torch.Tensor):
            s = scale.to(device)
            # If per-channel, ensure proper shape for broadcasting
            if s.dim() > 0 and s.numel() > 1:
                while s.dim() < x_q.dim():
                    s = s.unsqueeze(-1)
        else:
            s = torch.tensor(float(scale), device=device, dtype=x_q.dtype)
        
        # Compute quantization thresholds based on bit-width
        # For weights: typically symmetric quantization
        # Signed k-bit: [-2^(k-1), 2^(k-1)-1]
        thd_neg = -(1 << (k - 1))  # -2^(k-1)
        thd_pos = (1 << (k - 1)) - 1  # 2^(k-1)-1
        
        # ============================================================
        # æ­£ç¡®çš„æµç¨‹ï¼ˆç¬¦åˆç”¨æˆ·è¦æ±‚ï¼‰ï¼š
        # 1. æµ®ç‚¹æƒé‡ â†’ é‡åŒ–å™¨ â†’ æ•´æ•°ç ï¼ˆé€šè¿‡ round(x_q / s)ï¼‰
        # 2. æ•´æ•°ç  â†’ æ ¼é›·ç¼–ç ï¼ˆå¦‚æœä½¿ç”¨æ ¼é›·ç ï¼‰
        # 3. æ ¼é›·ç¼–ç ç©ºé—´ â†’ æ³¨å…¥æ•…éšœï¼ˆä½ç¿»è½¬ï¼‰
        # 4. æ ¼é›·ç¼–ç  â†’ æ˜ å°„å›æ•´æ•°ï¼ˆå¦‚æœä½¿ç”¨æ ¼é›·ç ï¼‰
        # 5. æ•´æ•° â†’ åé‡åŒ– â†’ æµ®ç‚¹
        # ============================================================
        
        # Step 1: ä»é‡åŒ–åçš„æµ®ç‚¹æ•°åæ¨æ•´æ•°ç 
        # LSQé‡åŒ–: x_q = round(x / s) * s
        # æ•´æ•°ç : code = round(x_q / s)ï¼ŒèŒƒå›´ [thd_neg, thd_pos]
        code_f = torch.round(x_q.to(device) / s)
        code_f = torch.clamp(code_f, thd_neg, thd_pos)
        
        # Shift to non-negative range [0, 2^k-1] for bit operations
        code_shifted = code_f - thd_neg  # Now in [0, 2^k-1]
        n_levels = (1 << k) - 1
        
        # Use compact integer dtype for efficiency
        code_dtype = torch.int16 if n_levels <= 32767 else torch.int32
        code = code_shifted.to(code_dtype).clamp(0, n_levels)
        
        # Step 2: å¦‚æœä½¿ç”¨æ ¼é›·ç æˆ–OLMï¼Œå°†æ•´æ•°ç è½¬æ¢ä¸ºç¼–ç ç©ºé—´
        if use_gray_code:
            import sys
            print(f"[DEBUG _inject] Step 2: Converting to gray code, code shape={code.shape}, device={code.device}, target device={device}", file=sys.stderr, flush=True)
            # ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šæ“ä½œ
            if code.device != device:
                code = code.to(device)
            # å‘é‡åŒ–ï¼šG = B ^ (B >> 1)
            code = code ^ (code >> 1)
            print(f"[DEBUG _inject] Step 2 completed: gray code shape={code.shape}, device={code.device}", file=sys.stderr, flush=True)
        elif use_olm:
            # OLMç¼–ç ï¼šå°†é‡åŒ–å€¼æ˜ å°„åˆ°ç¼–ç ç©ºé—´
            # éœ€è¦å…ˆå°†code_shiftedï¼ˆ0åˆ°n_levelsï¼‰è½¬æ¢å›åŸå§‹é‡åŒ–å€¼èŒƒå›´ï¼ˆthd_negåˆ°thd_posï¼‰
            code_original = code_shifted + thd_neg  # è½¬æ¢å›åŸå§‹é‡åŒ–å€¼èŒƒå›´
            value_to_code = self.olm_layers[layer_name]
            # ä½¿ç”¨å‘é‡åŒ–æŸ¥æ‰¾è¡¨è¿›è¡Œæ˜ å°„ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨GPU tensorä½œä¸ºæŸ¥æ‰¾è¡¨ï¼‰
            # åˆ›å»ºæŸ¥æ‰¾è¡¨ï¼šå°†é‡åŒ–å€¼èŒƒå›´æ˜ å°„åˆ°ç¼–ç 
            # æ³¨æ„ï¼šå¯¹äºæœªæ˜ å°„çš„å€¼ï¼Œä½¿ç”¨åŸå€¼ï¼ˆidentity mappingï¼‰
            lookup_table = torch.arange(n_levels + 1, dtype=code_dtype, device=device)  # é»˜è®¤identityæ˜ å°„
            for val, enc in value_to_code.items():
                idx = val - thd_neg  # è½¬æ¢åˆ°[0, n_levels]èŒƒå›´
                if 0 <= idx <= n_levels:
                    lookup_table[idx] = enc
            # å‘é‡åŒ–æŸ¥æ‰¾
            code = lookup_table[code_shifted.clamp(0, n_levels)]
        
        # Flatten for bit operations (GPU-accelerated)
        flat = code.view(-1)
        N = flat.numel()
        
        # Generate flip mask [N, k] using GPU
        # If layer_name is provided, generate fixed mask based on weight position
        # If forward_seed is provided (from seed_list), use it instead of self.seed
        mask_seed = forward_seed if forward_seed is not None else self.seed
        flip_mask = self._generate_flip_mask(N, k, device, layer_name=layer_name, mask_seed=mask_seed)
        
        # ç»Ÿè®¡å®é™…ç¿»è½¬çš„bitæ•°ï¼ˆå»¶è¿Ÿç»Ÿè®¡ï¼Œé¿å…GPU-CPUåŒæ­¥é˜»å¡ï¼‰
        # é»˜è®¤å…³é—­ç»Ÿè®¡åŠŸèƒ½ä»¥æå‡æ€§èƒ½ï¼Œéœ€è¦æ—¶å¯ä»¥é€šè¿‡enable_statistics=Trueå¯ç”¨
        if self.enable_statistics:
            total_bits = N * k
            total_params = N  # å‚æ•°æ€»æ•°ï¼ˆæ¯ä¸ªå‚æ•°æœ‰kä¸ªbitï¼‰
            # Use layer_name_for_stats if provided, otherwise fall back to layer_name or "unknown"
            stats_key = layer_name_for_stats if layer_name_for_stats is not None else (layer_name if layer_name is not None else "unknown")
            # å»¶è¿Ÿç»Ÿè®¡ï¼šåªå­˜å‚¨sumç»“æœï¼ˆtensorï¼Œä¸ç«‹å³åŒæ­¥åˆ°CPUï¼‰ï¼Œç¨åæ‰¹é‡å¤„ç†
            # è¿™æ ·å¯ä»¥é¿å…æ¯æ¬¡è°ƒç”¨éƒ½è§¦å‘GPU-CPUåŒæ­¥ï¼Œæå‡æ€§èƒ½
            flip_mask_sum = flip_mask.sum()  # è¿”å›tensorï¼Œä¸è°ƒç”¨.item()
            # è®¡ç®—å—å½±å“çš„å‚æ•°æ•°é‡ï¼šè‡³å°‘æœ‰ä¸€ä¸ªbitè¢«ç¿»è½¬çš„å‚æ•°
            # flip_maskå½¢çŠ¶ä¸º[N, k]ï¼Œå¯¹æ¯è¡Œæ±‚å’Œï¼Œå¦‚æœ>0è¯´æ˜è¯¥å‚æ•°è‡³å°‘æœ‰ä¸€ä¸ªbitè¢«ç¿»è½¬
            affected_params_sum = (flip_mask.sum(dim=1) > 0).sum()  # è¿”å›tensorï¼Œä¸è°ƒç”¨.item()
            self._pending_stats.append((flip_mask_sum, total_bits, total_params, stats_key, affected_params_sum))
        
        # Step 3: åœ¨ç¼–ç ç©ºé—´ï¼ˆäºŒè¿›åˆ¶æˆ–æ ¼é›·ç ï¼‰ä¸­è¿›è¡Œä½ç¿»è½¬ï¼ˆå‘é‡åŒ–ä¼˜åŒ–ï¼‰
        # é¢„è®¡ç®—ä½æƒé‡ï¼Œé¿å…é‡å¤è®¡ç®—ï¼Œæå‡æ€§èƒ½
        bit_positions = torch.arange(k, device=device, dtype=torch.int64)
        bit_weights = (1 << bit_positions).to(torch.int64)  # é¢„è®¡ç®—ä½æƒé‡ï¼Œé¿å…é‡å¤ä½ç§»
        
        # å‘é‡åŒ–ä½æå–å’Œç¿»è½¬ï¼ˆå‡å°‘ç±»å‹è½¬æ¢ï¼Œç¡®ä¿è®¾å¤‡ä¸€è‡´ï¼‰
        flat_int64 = flat.to(torch.int64)
        if flat_int64.device != device:
            flat_int64 = flat_int64.to(device)
        bits = ((flat_int64.unsqueeze(-1) >> bit_positions) & 1).to(torch.bool)
        flipped_bits = bits ^ flip_mask
        
        # å‘é‡åŒ–é‡å»ºç¼–ç å€¼ï¼ˆä½¿ç”¨é¢„è®¡ç®—çš„æƒé‡ï¼Œé¿å…é‡å¤ä½ç§»æ“ä½œï¼‰
        flat_faulted = (flipped_bits.to(torch.int64) * bit_weights).sum(-1)
        flat_faulted = flat_faulted.clamp(0, n_levels).to(code_dtype)
        # ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if flat_faulted.device != device:
            flat_faulted = flat_faulted.to(device)
        
        # Step 4: å¦‚æœä½¿ç”¨æ ¼é›·ç æˆ–OLMï¼Œå°†ç¼–ç è½¬æ¢å›äºŒè¿›åˆ¶æ•´æ•°ç 
        if use_gray_code:
            import sys
            print(f"[DEBUG _inject] Step 4: Converting gray to binary, k={k}, flat_faulted shape={flat_faulted.shape}, device={flat_faulted.device}, target device={device}", file=sys.stderr, flush=True)
            # ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šæ“ä½œ
            if flat_faulted.device != device:
                flat_faulted = flat_faulted.to(device)
            # å‘é‡åŒ–è½¬æ¢ï¼šB = G ^ (G >> 1) ^ (G >> 2) ^ ... ^ (G >> (k-1))
            # éœ€è¦ä¿ç•™åŸå§‹å€¼ï¼Œä½†ä½¿ç”¨æ›´é«˜æ•ˆçš„æ–¹å¼ï¼šç´¯ç§¯å¼‚æˆ–ï¼Œé¿å…å¤šæ¬¡å†…å­˜åˆ†é…
            gray_orig = flat_faulted  # ä¿å­˜åŸå§‹æ ¼é›·ç å€¼
            # å¯¹äºå¸¸è§çš„å°ä½å®½ï¼ˆ2-8 bitï¼‰ï¼Œç›´æ¥å±•å¼€å¾ªç¯
            binary = gray_orig
            if k >= 2:
                binary = binary ^ (gray_orig >> 1)
            if k >= 3:
                binary = binary ^ (gray_orig >> 2)
            if k >= 4:
                binary = binary ^ (gray_orig >> 3)
            if k >= 5:
                binary = binary ^ (gray_orig >> 4)
            if k >= 6:
                binary = binary ^ (gray_orig >> 5)
            if k >= 7:
                binary = binary ^ (gray_orig >> 6)
            if k >= 8:
                binary = binary ^ (gray_orig >> 7)
            # å¯¹äºæ›´å¤§çš„ä½å®½ï¼ˆå¾ˆå°‘è§ï¼‰ï¼Œä½¿ç”¨å¾ªç¯
            if k > 8:
                for i in range(8, min(k, 16)):
                    binary = binary ^ (gray_orig >> i)
            flat_faulted = binary
            print(f"[DEBUG _inject] Step 4 completed: binary shape={flat_faulted.shape}, device={flat_faulted.device}", file=sys.stderr, flush=True)
        elif use_olm:
            # OLMè§£ç ï¼šå°†ç¼–ç æ˜ å°„å›é‡åŒ–å€¼
            code_to_value = self.olm_code_to_value[layer_name]
            # ä½¿ç”¨å‘é‡åŒ–æŸ¥æ‰¾è¡¨è¿›è¡Œæ˜ å°„ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨GPU tensorä½œä¸ºæŸ¥æ‰¾è¡¨ï¼‰
            # åˆ›å»ºåå‘æŸ¥æ‰¾è¡¨ï¼šå°†ç¼–ç æ˜ å°„å›é‡åŒ–å€¼
            max_code = max(code_to_value.keys()) if code_to_value else n_levels
            # æ³¨æ„ï¼šå¯¹äºæœªæ˜ å°„çš„ç¼–ç ï¼Œä½¿ç”¨åŸå€¼ï¼ˆidentity mappingï¼‰
            reverse_lookup = torch.arange(max_code + 1, dtype=code_dtype, device=device)  # é»˜è®¤identityæ˜ å°„
            for enc, val in code_to_value.items():
                if 0 <= enc <= max_code:
                    reverse_lookup[enc] = val - thd_neg  # è½¬æ¢åˆ°[0, n_levels]èŒƒå›´
            # å‘é‡åŒ–æŸ¥æ‰¾
            flat_faulted = reverse_lookup[flat_faulted.clamp(0, max_code)].clamp(0, n_levels)
        
        # Reshape back to original shape
        code_faulted = flat_faulted.view_as(code)
        
        # Step 5: å°†æ•´æ•°ç è½¬æ¢å›é‡åŒ–èŒƒå›´å¹¶åé‡åŒ–å›æµ®ç‚¹æ•°
        # Shift back to original range [thd_neg, thd_pos]
        code_faulted_shifted = code_faulted.to(x_q.dtype) + thd_neg
        
        # De-quantize back to float: x_faulted = code_faulted * s
        x_faulted = code_faulted_shifted * s
        
        # Numerical safety: clamp to reasonable range
        if torch.is_floating_point(x_faulted):
            max_range = torch.abs(s) * (thd_pos + 1)
            x_faulted = torch.clamp(x_faulted, -max_range, max_range)
        
        return x_faulted
    
    def _generate_flip_mask(self, N: int, k: int, device: torch.device, layer_name: Optional[str] = None, mask_seed: Optional[int] = None) -> torch.Tensor:
        """
        Generate a boolean tensor of shape [N, k] indicating which bits to flip.
        Uses GPU-accelerated random generation.
        
        If layer_name is provided, generates fixed mask based on weight position,
        ensuring the same weight position always gets the same mask (for reproducibility
        between training and validation).
        
        Args:
            N: Number of elements
            k: Bit-width
            device: Device for tensor generation
            layer_name: Optional layer name for generating position-based fixed mask
            mask_seed: Optional seed for generating mask (if None, use self.seed or random)
            
        Returns:
            Boolean tensor [N, k] where True indicates bit should be flipped
        """
        if self.mode == "ber":
            p = float(self.ber or 0.0)
            
            # Use mask_seed if provided, otherwise use self.seed
            seed_to_use = mask_seed if mask_seed is not None else self.seed
            
            # Only use position-based mask if explicitly enabled AND layer_name is provided
            # Otherwise, use fast random mask generation (even if layer_name is provided for statistics)
            if layer_name is not None and self.use_position_based_mask:
                # Generate fixed mask based on weight position
                # This ensures the same weight position always gets the same mask
                # across different forward passes (training vs validation)
                import hashlib
                
                # Use hash function to generate fixed mask (GPU-friendly, deterministic)
                # For each weight position i and bit j, compute hash and map to [0, 1]
                # This is more efficient than using torch.Generator in a loop
                mask = torch.zeros((N, k), dtype=torch.bool, device=device)
                
                # Vectorized approach: generate all position indices
                i_indices = torch.arange(N, device=device, dtype=torch.int64)
                j_indices = torch.arange(k, device=device, dtype=torch.int64)
                
                # Create meshgrid for all (i, j) combinations
                i_grid, j_grid = torch.meshgrid(i_indices, j_indices, indexing='ij')
                i_flat = i_grid.flatten()
                j_flat = j_grid.flatten()
                
                # Generate hash-based random values for all positions at once
                # Convert to CPU for hash computation (hashlib doesn't support GPU)
                i_cpu = i_flat.cpu().numpy()
                j_cpu = j_flat.cpu().numpy()
                
                # Compute hash for each position and map to [0, 1]
                hash_values = []
                for idx in range(len(i_cpu)):
                    i_val, j_val = int(i_cpu[idx]), int(j_cpu[idx])
                    # Create unique identifier for this weight position and bit
                    # Use seed_to_use instead of self.seed
                    identifier = f"{layer_name}_{i_val}_{j_val}_{seed_to_use}"
                    # Compute hash
                    hash_int = int(hashlib.md5(identifier.encode()).hexdigest()[:8], 16)
                    # Map to [0, 1] range
                    hash_val = (hash_int % 1000000) / 1000000.0
                    hash_values.append(hash_val)
                
                # Convert to tensor and reshape
                hash_tensor = torch.tensor(hash_values, device=device, dtype=torch.float32)
                mask_flat = hash_tensor < p
                mask = mask_flat.reshape(N, k)
                
                return mask
            else:
                # Generate random mask using seed_to_use
                if seed_to_use is not None:
                    # Use generator with specific seed for reproducibility
                    generator = torch.Generator(device=device)
                    generator.manual_seed(seed_to_use)
                    return torch.rand((N, k), generator=generator, device=device) < p
                else:
                    # Original behavior: generate random mask each time (no seed)
                    # GPU-accelerated: generate all random values at once
                    return torch.rand((N, k), device=device) < p
        else:
            raise ValueError(f"Unsupported mode: {self.mode}")


def load_bit_width_config_from_json(json_path: str, config_index: int = 0) -> Tuple[List[int], List[int]]:
    """
    Load bit-width configuration from JSON file generated by search.
    
    Args:
        json_path: Path to JSON configuration file (e.g., from search output)
        config_index: Index of configuration to use (default: 0, use first configuration)
        
    Returns:
        Tuple of (weight_bits, act_bits) lists for each quantized layer
        
    Example:
        >>> weight_bits, act_bits = load_bit_width_config_from_json(
        ...     "search/resnet18_cifar10_single_gpu_search_bit_width_config.json"
        ... )
        >>> set_bit_width(model, weight_bits, act_bits)
    """
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"Bit-width config file not found: {json_path}")
    
    with open(json_path, 'r') as f:
        config_data = json.load(f)
    
    if 'configurations' not in config_data:
        raise ValueError(f"Invalid config file format: 'configurations' key not found")
    
    if len(config_data['configurations']) == 0:
        raise ValueError(f"No configurations found in config file")
    
    if config_index >= len(config_data['configurations']):
        raise ValueError(
            f"Config index {config_index} out of range. "
            f"File contains {len(config_data['configurations'])} configurations."
        )
    
    config = config_data['configurations'][config_index]
    
    weight_bits = config['weight_bits']
    act_bits = config['act_bits']
    
    # Convert to Python int if needed
    weight_bits = [int(b) for b in weight_bits]
    act_bits = [int(b) for b in act_bits]
    
    if len(weight_bits) != len(act_bits):
        raise ValueError(
            f"Mismatch in bit-width lists: "
            f"weight_bits has {len(weight_bits)} elements, "
            f"act_bits has {len(act_bits)} elements"
        )
    
    return weight_bits, act_bits


def setup_model_with_bit_width_config(
    model: torch.nn.Module,
    json_path: str,
    config_index: int = 0,
    verbose: bool = True
) -> Tuple[List[int], List[int]]:
    """
    Load bit-width configuration from JSON and set it on the model.
    
    This is a convenience function that combines loading and setting bit-widths.
    It should be called before enabling fault injection for mixed-precision models.
    
    Args:
        model: The quantized model
        json_path: Path to JSON configuration file from search
        config_index: Index of configuration to use (default: 0)
        verbose: Whether to print information about loaded configuration
        
    Returns:
        Tuple of (weight_bits, act_bits) that were set on the model
        
    Example:
        >>> setup_model_with_bit_width_config(
        ...     model,
        ...     "search/resnet18_cifar10_single_gpu_search_bit_width_config.json"
        ... )
        >>> injector = FaultInjector(model, mode="ber", ber=1e-6)
        >>> injector.enable()
    """
    weight_bits, act_bits = load_bit_width_config_from_json(json_path, config_index)
    
    # Get quantized layers, excluding those with fixed_bits (first/last layers)
    # The config file only contains bit-widths for layers without fixed_bits
    quantized_layers = []
    for name, module in model.named_modules():
        if isinstance(module, (QuanConv2d, QuanLinear)):
            # Skip layers with fixed_bits (typically first conv and last linear)
            # These layers are not included in the search config
            if hasattr(module, 'fixed_bits') and module.fixed_bits is not None:
                continue
            quantized_layers.append(module)
    
    # Handle layer count mismatch more gracefully
    if len(quantized_layers) != len(weight_bits):
        if len(weight_bits) > len(quantized_layers):
            # Config has more layers than model - use only the first N layers from config
            if verbose:
                print(f"  Warning: Config specifies {len(weight_bits)} layers, but model has {len(quantized_layers)} layers.")
                print(f"  Using first {len(quantized_layers)} layers from config.")
            weight_bits = weight_bits[:len(quantized_layers)]
            act_bits = act_bits[:len(quantized_layers)]
        else:
            # Config has fewer layers than model - only set first M layers, warn about the rest
            if verbose:
                print(f"  Warning: Model has {len(quantized_layers)} layers, but config specifies {len(weight_bits)} layers.")
                print(f"  Only setting bit-widths for first {len(weight_bits)} layers. Remaining layers will keep their current bit-widths.")
            # We'll only set the first len(weight_bits) layers
    
    # Set bit-widths on the model (only for layers without fixed_bits)
    # We need to also get BN layers for switching
    from .qat import get_quantized_layers
    
    # First, set bits on all layers so get_quantized_layers can find them
    # We'll set them directly first, then use set_bit_width to also update BN layers
    config_idx = 0
    layers_set = 0
    for name, module in model.named_modules():
        if isinstance(module, (QuanConv2d, QuanLinear)):
            # Skip layers with fixed_bits
            if hasattr(module, 'fixed_bits') and module.fixed_bits is not None:
                continue
            # Set bits for this layer (only if we have config for it)
            if config_idx < len(weight_bits):
                module.bits = (weight_bits[config_idx], act_bits[config_idx])
                layers_set += 1
                config_idx += 1
    
    # Now get quantized layers and BN layers to update BN
    try:
        layers, bns = get_quantized_layers(model)
        # Update BN layers for the layers we actually set
        for idx in range(min(layers_set, len(layers))):
            if idx < len(bns) and bns[idx] is not None:
                if hasattr(bns[idx], 'switch_bn'):
                    bns[idx].switch_bn(layers[idx].bits)
    except Exception as e:
        # If get_quantized_layers fails (e.g., output_size not set), continue anyway
        # The bits are already set on the layers
        if verbose:
            print(f"  Warning: Could not update BN layers: {e}")
    
    # Return the actual bits that were set (may be truncated if config had more layers)
    actual_weight_bits = weight_bits[:layers_set] if layers_set < len(weight_bits) else weight_bits
    actual_act_bits = act_bits[:layers_set] if layers_set < len(act_bits) else act_bits
    
    if verbose:
        print(f"Loaded bit-width configuration from: {json_path}")
        print(f"  Configuration index: {config_index}")
        print(f"  Set bit-widths on {layers_set} out of {len(quantized_layers)} layers")
        if layers_set < len(quantized_layers):
            print(f"  Note: {len(quantized_layers) - layers_set} layers were not configured (keeping current bit-widths)")
        if actual_weight_bits:
            print(f"  Weight bits range: {min(actual_weight_bits)}-{max(actual_weight_bits)}")
            print(f"  Activation bits range: {min(actual_act_bits)}-{max(actual_act_bits)}")
            print(f"  Sample weight bits (first 5 layers): {actual_weight_bits[:5]}")
            print(f"  Sample act bits (first 5 layers): {actual_act_bits[:5]}")
    
    return actual_weight_bits, actual_act_bits

