# Evaluation config for alexnet_cifar10_FAT_KL_entropy_progressive_ber
name: alexnet_cifar10_FAT_KL_entropy_progressive_ber_eval

# Name of output directory. Checkpoints and logs will be saved at `pwd`/output_dir
output_dir: eval
training_device: gpu

target_bits: [6, 5, 4, 3, 2]
post_training_batchnorm_calibration: false
information_distortion_mitigation: false
enable_dynamic_bit_training: true

# Dataset loader
dataloader: 
  dataset: cifar10
  num_classes: 10
  path: ./data/cifar10
  batch_size: 128
  workers: 4
  deterministic: true

resume:
  path: training/alexnet_cifar10_FAT_KL_entropy_progressive_ber/alexnet_cifar10_FAT_KL_entropy_progressive_ber_checkpoint.pth.tar
  lean: false

log:
  num_best_scores: 3
  print_freq: 20

#============================ Model ============================================

arch: alexnet
pre_trained: false

#============================ Quantization =====================================

# (default for all layers)
quan:
  act: 
    mode: lsq
    bit: 2
    per_channel: false
    symmetric: false
    all_positive: true

  weight: 
    mode: lsq
    bit: 2
    per_channel: false
    symmetric: false
    all_positive: false
  
  excepts:
    excepts_bits_width: 8
    # classifier.1 uses dynamic bits [2,3,4,5,6] (NOT in excepts)
    classifier.1: __delete__
    features.0.0: __delete__  # Remove MobileNetV2 specific keys
    features.0.1: __delete__
    features.0:  # First conv layer
      act:
        bit: 
        all_positive: false
      weight:
        bit:
    classifier.6:  # Last linear layer
      act:
        bit:
        all_positive: false
      weight:
        bit:

#============================ Training / Evaluation ============================

search: false
eval: true

epochs: 1
smoothing: 0.0
scale_gradient: false

opt: sgd
lr: 0.01
momentum: 0.9
weight_decay: 0.0001
adaptive_region_weight_decay: 0

sched: cosine
min_lr: 0.0
decay_rate: 0.1
warmup_epochs: 0
warmup_lr: 0.0
decay_epochs: 30
cooldown_epochs: 0

ema_decay: 0.9997

# Distributed training settings (for single GPU)
distributed: false
local_rank: 0
rank: 0
world_size: 1
device: cuda:0

